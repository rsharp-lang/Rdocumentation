<!DOCTYPE html><html lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0" />
    <title>CNN</title>
    <meta name="author" content="xie.guigang@gcmodeller.org" />
    <meta name="copyright" content="SMRUCC genomics Copyright (c) 2022" />
    <meta name="keywords" content="R#; CNN; MLkit" />
    <meta name="generator" content="https://github.com/rsharp-lang" />
    <meta name="theme-color" content="#333" />
    <meta name="description" content="feed-forward phase of deep Convolutional Neural Networks..." />
    <meta class="foundation-data-attribute-namespace" />
    <meta class="foundation-mq-xxlarge" />
    <meta class="foundation-mq-xlarge" />
    <meta class="foundation-mq-large" />
    <meta class="foundation-mq-medium" />
    <meta class="foundation-mq-small" />
    <meta class="foundation-mq-topbar" />
    <style>

.table-three-line {
border-collapse:collapse; /* 关键属性：合并表格内外边框(其实表格边框有2px，外面1px，里面还有1px哦) */
border:solid #000000; /* 设置边框属性；样式(solid=实线)、颜色(#999=灰) */
border-width:2px 0 2px 0px; /* 设置边框状粗细：上 右 下 左 = 对应：1px 0 0 1px */
}
.left-1{
    border:solid #000000;border-width:1px 1px 2px 0px;padding:2px;
    font-weight:bolder;
}
.right-1{
    border:solid #000000;border-width:1px 0px 2px 1px;padding:2px;
    font-weight:bolder;
}
.mid-1{
    border:solid #000000;border-width:1px 1px 2px 1px;padding:2px;
    font-weight:bolder;
}
.left{
    border:solid #000000;border-width:1px 1px 1px 0px;padding:2px;
}
.right{
    border:solid #000000;border-width:1px 0px 1px 1px;padding:2px;
}
.mid{
    border:solid #000000;border-width:1px 1px 1px 1px;padding:2px;
}
table caption {font-size:14px;font-weight:bolder;}
</style>
  </head>
  <body>
    <table width="100%" summary="page for {CNN}">
      <tbody>
        <tr>
          <td>{CNN}</td>
          <td style="text-align: right;">R# Documentation</td>
        </tr>
      </tbody>
    </table>
    <h1>CNN</h1>
    <hr />
    <p style="     font-size: 1.125em;     line-height: .8em;     margin-left: 0.5%;     background-color: #fbfbfb;     padding: 24px; ">
      <code>
        <span style="color: blue;">require</span>(<span style="color: black; font-weight: bold;">R</span>);
                               <br /><br /><span style="color: green;">#' feed-forward phase of deep Convolutional Neural Networks</span><br /><span style="color: blue;">imports</span><span style="color: brown"> "CNN"</span><span style="color: blue;"> from</span><span style="color: brown"> "MLkit"</span>;
                           </code>
    </p>
    <p><p>feed-forward phase of deep Convolutional Neural Networks</p></p>
    <blockquote>
      <p style="font-style: italic; font-size: 0.9em;">
                           <p>feed-forward phase of deep Convolutional Neural Networks</p>
                           </p>
    </blockquote>
    <div id="main-wrapper">
      <table class="table-three-line" style="display: block">
        <caption>.NET clr type export</caption>
        <tbody><tr>
  <td id="cnn">
    <a href="/vignettes/clr/Microsoft/VisualBasic/MachineLearning/CNN/LayerBuilder.html">cnn: LayerBuilder</a>
  </td>
  <td></td>
</tr></tbody>
      </table>
      <br />
      <br />
      <table class="table-three-line">
        <caption>.NET clr function exports</caption>
        <tbody><tr>
  <td id="n_threads">
    <a href="./CNN/n_threads.html">n_threads</a>
  </td>
  <td><p>get/set of the CNN parallel thread number</p></td>
</tr>
<tr>
  <td id="cnn">
    <a href="./CNN/cnn.html">cnn</a>
  </td>
  <td><p>Create a new CNN model
 
 Convolutional neural network (CNN) is a regularized type of feed-forward
 neural network that learns feature engineering by itself via filters 
 (or kernel) optimization. Vanishing gradients and exploding gradients, 
 seen during backpropagation in earlier neural networks, are prevented by 
 using regularized weights over fewer connections.</p></td>
</tr>
<tr>
  <td id="input_layer">
    <a href="./CNN/input_layer.html">input_layer</a>
  </td>
  <td><p>The input layer is a simple layer that will pass the data though and
 create a window into the full training data set. So for instance if
 we have an image of size 28x28x1 which means that we have 28 pixels
 in the x axle and 28 pixels in the y axle and one color (gray scale),
 then this layer might give you a window of another size example 24x24x1
 that is randomly chosen in order to create some distortion into the
 dataset so the algorithm don't over-fit the training.</p></td>
</tr>
<tr>
  <td id="regression_layer">
    <a href="./CNN/regression_layer.html">regression_layer</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="conv_layer">
    <a href="./CNN/conv_layer.html">conv_layer</a>
  </td>
  <td><p>This layer uses different filters to find attributes of the data that
 affects the result. As an example there could be a filter to find
 horizontal edges in an image.</p></td>
</tr>
<tr>
  <td id="conv_transpose_layer">
    <a href="./CNN/conv_transpose_layer.html">conv_transpose_layer</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="lrn_layer">
    <a href="./CNN/lrn_layer.html">lrn_layer</a>
  </td>
  <td><p>This layer is useful when we are dealing with ReLU neurons. Why is that?
 Because ReLU neurons have unbounded activations and we need LRN to normalize
 that. We want to detect high frequency features with a large response. If we
 normalize around the local neighborhood of the excited neuron, it becomes even
 more sensitive as compared to its neighbors.
 
 At the same time, it will dampen the responses that are uniformly large in any
 given local neighborhood. If all the values are large, then normalizing those
 values will diminish all of them. So basically we want to encourage some kind
 of inhibition and boost the neurons with relatively larger activations. This
 has been discussed nicely in Section 3.3 of the original paper by Krizhevsky et al.</p></td>
</tr>
<tr>
  <td id="tanh_layer">
    <a href="./CNN/tanh_layer.html">tanh_layer</a>
  </td>
  <td><p>Implements Tanh nonlinearity elementwise x to tanh(x)
 so the output is between -1 and 1.</p></td>
</tr>
<tr>
  <td id="softmax_layer">
    <a href="./CNN/softmax_layer.html">softmax_layer</a>
  </td>
  <td><p>[*loss_layers] This layer will squash the result of the activations in the fully
 connected layer and give you a value of 0 to 1 for all output activations.</p></td>
</tr>
<tr>
  <td id="relu_layer">
    <a href="./CNN/relu_layer.html">relu_layer</a>
  </td>
  <td><p>This is a layer of neurons that applies the non-saturating activation
 function f(x)=max(0,x). It increases the nonlinear properties of the
 decision function and of the overall network without affecting the
 receptive fields of the convolution layer.</p></td>
</tr>
<tr>
  <td id="leaky_relu_layer">
    <a href="./CNN/leaky_relu_layer.html">leaky_relu_layer</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="maxout_layer">
    <a href="./CNN/maxout_layer.html">maxout_layer</a>
  </td>
  <td><p>Implements Maxout nonlinearity that computes x to max(x)
 where x is a vector of size group_size. Ideally of course,
 the input size should be exactly divisible by group_size</p></td>
</tr>
<tr>
  <td id="sigmoid_layer">
    <a href="./CNN/sigmoid_layer.html">sigmoid_layer</a>
  </td>
  <td><p>Implements Sigmoid nonlinearity elementwise x to 1/(1+e^(-x))
 so the output is between 0 and 1.</p></td>
</tr>
<tr>
  <td id="pool_layer">
    <a href="./CNN/pool_layer.html">pool_layer</a>
  </td>
  <td><p>This layer will reduce the dataset by creating a smaller zoomed out
 version. In essence you take a cluster of pixels take the sum of them
 and put the result in the reduced position of the new image.</p></td>
</tr>
<tr>
  <td id="dropout_layer">
    <a href="./CNN/dropout_layer.html">dropout_layer</a>
  </td>
  <td><p>This layer will remove some random activations in order to
 defeat over-fitting.</p></td>
</tr>
<tr>
  <td id="full_connected_layer">
    <a href="./CNN/full_connected_layer.html">full_connected_layer</a>
  </td>
  <td><p>Neurons in a fully connected layer have full connections to all
 activations in the previous layer, as seen in regular Neural Networks.
 Their activations can hence be computed with a matrix multiplication
 followed by a bias offset.</p></td>
</tr>
<tr>
  <td id="gaussian_layer">
    <a href="./CNN/gaussian_layer.html">gaussian_layer</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="sample_dataset">
    <a href="./CNN/sample_dataset.html">sample_dataset</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="sample_dataset.image">
    <a href="./CNN/sample_dataset.image.html">sample_dataset.image</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="auto_encoder">
    <a href="./CNN/auto_encoder.html">auto_encoder</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="training">
    <a href="./CNN/training.html">training</a>
  </td>
  <td><p>Do CNN network model training</p></td>
</tr>
<tr>
  <td id="ada_delta">
    <a href="./CNN/ada_delta.html">ada_delta</a>
  </td>
  <td><p>Adaptive delta will look at the differences between the expected result and the current result to train the network.</p></td>
</tr>
<tr>
  <td id="ada_grad">
    <a href="./CNN/ada_grad.html">ada_grad</a>
  </td>
  <td><p>The adaptive gradient trainer will over time sum up the square of
 the gradient and use it to change the weights.</p></td>
</tr>
<tr>
  <td id="adam">
    <a href="./CNN/adam.html">adam</a>
  </td>
  <td><p>Adaptive Moment Estimation is an update to RMSProp optimizer. In this running average of both the
 gradients and their magnitudes are used.</p></td>
</tr>
<tr>
  <td id="nesterov">
    <a href="./CNN/nesterov.html">nesterov</a>
  </td>
  <td><p>Another extension of gradient descent is due to Yurii Nesterov from 1983,[7] and has been subsequently generalized</p></td>
</tr>
<tr>
  <td id="sgd">
    <a href="./CNN/sgd.html">sgd</a>
  </td>
  <td><p>Stochastic gradient descent (often shortened in SGD), also known as incremental gradient descent, is a
 stochastic approximation of the gradient descent optimization method for minimizing an objective function
 that is written as a sum of differentiable functions. In other words, SGD tries to find minimums or
 maximums by iteration.</p></td>
</tr>
<tr>
  <td id="window_grad">
    <a href="./CNN/window_grad.html">window_grad</a>
  </td>
  <td><p>This is AdaGrad but with a moving window weighted average
 so the gradient is not accumulated over the entire history of the run.
 it's also referred to as Idea #1 in Zeiler paper on AdaDelta.</p></td>
</tr>
<tr>
  <td id="predict">
    <a href="./CNN/predict.html">predict</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="CeNiN">
    <a href="./CNN/CeNiN.html">CeNiN</a>
  </td>
  <td><p>load a CNN model from file</p></td>
</tr>
<tr>
  <td id="detectObject">
    <a href="./CNN/detectObject.html">detectObject</a>
  </td>
  <td><p>classify a object from a given image data</p></td>
</tr>
<tr>
  <td id="saveModel">
    <a href="./CNN/saveModel.html">saveModel</a>
  </td>
  <td><p>save the CNN model into a binary data file</p></td>
</tr></tbody>
      </table>
    </div>
    <hr />
    <div style="text-align: center;">[<a href="../index.html">Document Index</a>]</div>
  </body>
</html>