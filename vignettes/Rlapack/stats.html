<!DOCTYPE html><html lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0" />
    <title>stats</title>
    <meta name="author" content="xie.guigang@gcmodeller.org" />
    <meta name="copyright" content="SMRUCC genomics Copyright (c) 2022" />
    <meta name="keywords" content="R#; stats; Rlapack" />
    <meta name="generator" content="https://github.com/rsharp-lang" />
    <meta name="theme-color" content="#333" />
    <meta name="description" content="The R Stats Package R statistical functions, This package contai..." />
    <meta class="foundation-data-attribute-namespace" />
    <meta class="foundation-mq-xxlarge" />
    <meta class="foundation-mq-xlarge" />
    <meta class="foundation-mq-large" />
    <meta class="foundation-mq-medium" />
    <meta class="foundation-mq-small" />
    <meta class="foundation-mq-topbar" />
    <style>

.table-three-line {
border-collapse:collapse; /* 关键属性：合并表格内外边框(其实表格边框有2px，外面1px，里面还有1px哦) */
border:solid #000000; /* 设置边框属性；样式(solid=实线)、颜色(#999=灰) */
border-width:2px 0 2px 0px; /* 设置边框状粗细：上 右 下 左 = 对应：1px 0 0 1px */
}
.left-1{
    border:solid #000000;border-width:1px 1px 2px 0px;padding:2px;
    font-weight:bolder;
}
.right-1{
    border:solid #000000;border-width:1px 0px 2px 1px;padding:2px;
    font-weight:bolder;
}
.mid-1{
    border:solid #000000;border-width:1px 1px 2px 1px;padding:2px;
    font-weight:bolder;
}
.left{
    border:solid #000000;border-width:1px 1px 1px 0px;padding:2px;
}
.right{
    border:solid #000000;border-width:1px 0px 1px 1px;padding:2px;
}
.mid{
    border:solid #000000;border-width:1px 1px 1px 1px;padding:2px;
}
table caption {font-size:14px;font-weight:bolder;}
</style>
  </head>
  <body>
    <table width="100%" summary="page for {stats}">
      <tbody>
        <tr>
          <td>{stats}</td>
          <td style="text-align: right;">R# Documentation</td>
        </tr>
      </tbody>
    </table>
    <h1>stats</h1>
    <hr />
    <p style="     font-size: 1.125em;     line-height: .8em;     margin-left: 0.5%;     background-color: #fbfbfb;     padding: 24px; ">
      <code>
        <span style="color: blue;">require</span>(<span style="color: black; font-weight: bold;">R</span>);
                               <br /><br /><span style="color: green;">#' The R Stats Package</span><br /><span style="color: blue;">imports</span><span style="color: brown"> "stats"</span><span style="color: blue;"> from</span><span style="color: brown"> "Rlapack"</span>;
                           </code>
    </p>
    <p><h3>The R Stats Package</h3>

<p>R statistical functions, This package contains 
 functions for statistical calculations and random 
 number generation.
 
 For a complete list of functions, use <code>library(help = "stats")</code>.</p></p>
    <blockquote>
      <p style="font-style: italic; font-size: 0.9em;">
                           <h3>The R Stats Package</h3>

<p>R statistical functions, This package contains 
 functions for statistical calculations and random 
 number generation.
 
 For a complete list of functions, use <code>library(help = "stats")</code>.</p>
                           </p>
    </blockquote>
    <div id="main-wrapper">
      <table class="table-three-line" style="display: none">
        <caption>.NET clr type export</caption>
        <tbody></tbody>
      </table>
      <br />
      <br />
      <table class="table-three-line">
        <caption>.NET clr function exports</caption>
        <tbody><tr>
  <td id="combin">
    <a href="./stats/combin.html">combin</a>
  </td>
  <td><p>calculates <code>C(n, k)</code>.</p></td>
</tr>
<tr>
  <td id="pnorm">
    <a href="./stats/pnorm.html">pnorm</a>
  </td>
  <td><p>The Normal Distribution
 
 Density, distribution function, quantile function and random generation 
 for the normal distribution with mean equal to mean and standard deviation 
 equal to sd.</p></td>
</tr>
<tr>
  <td id="dnorm">
    <a href="./stats/dnorm.html">dnorm</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="p.adjust">
    <a href="./stats/p.adjust.html">p.adjust</a>
  </td>
  <td><h3>Adjust P-values for Multiple Comparisons</h3>

<p>Given a set of p-values, returns p-values adjusted 
 using one of several methods.</p></td>
</tr>
<tr>
  <td id="ecdf">
    <a href="./stats/ecdf.html">ecdf</a>
  </td>
  <td><h2>Empirical Cumulative Distribution Function</h2>

<p>Compute an empirical cumulative distribution function, with several methods for 
 plotting, printing and computing with such an “ecdf” object.</p></td>
</tr>
<tr>
  <td id="CDF">
    <a href="./stats/CDF.html">CDF</a>
  </td>
  <td><h2>Empirical Cumulative Distribution Function</h2>

<p>Compute an empirical cumulative distribution function</p></td>
</tr>
<tr>
  <td id="spline">
    <a href="./stats/spline.html">spline</a>
  </td>
  <td><p>Interpolating Splines</p></td>
</tr>
<tr>
  <td id="tabulate.mode">
    <a href="./stats/tabulate.mode.html">tabulate.mode</a>
  </td>
  <td><p>Average by removes outliers</p></td>
</tr>
<tr>
  <td id="prcomp">
    <a href="./stats/prcomp.html">prcomp</a>
  </td>
  <td><h3>Principal Components Analysis</h3>

<p>Performs a principal components analysis on the given data matrix 
 and returns the results as an object of class <code>prcomp</code>.
 
 The calculation is done by a singular value decomposition of the 
 (centered and possibly scaled) data matrix, not by using eigen on 
 the covariance matrix. This is generally the preferred method for 
 numerical accuracy. The print method for these objects prints the 
 results in a nice format and the plot method produces a scree 
 plot.</p>

<p>Unlike princomp, variances are computed With the usual divisor N - 1.
 Note that scale = True cannot be used If there are zero Or constant 
 (For center = True) variables.</p></td>
</tr>
<tr>
  <td id="as.dist">
    <a href="./stats/as.dist.html">as.dist</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="corr">
    <a href="./stats/corr.html">corr</a>
  </td>
  <td><p>matrix correlation</p></td>
</tr>
<tr>
  <td id="corr_sign">
    <a href="./stats/corr_sign.html">corr_sign</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="corr.test">
    <a href="./stats/corr.test.html">corr.test</a>
  </td>
  <td><p>Find the correlations, sample sizes, and probability 
 values between elements of a matrix or data.frame.
 
 Although the cor function finds the correlations for 
 a matrix, it does not report probability values. cor.test 
 does, but for only one pair of variables at a time. 
 corr.test uses cor to find the correlations for either 
 complete or pairwise data and reports the sample sizes 
 and probability values as well. For symmetric matrices, 
 raw probabilites are reported below the diagonal and 
 correlations adjusted for multiple comparisons above the 
 diagonal. In the case of different x and ys, the default 
 is to adjust the probabilities for multiple tests. Both 
 corr.test and corr.p return raw and adjusted confidence 
 intervals for each correlation.</p></td>
</tr>
<tr>
  <td id="quantile">
    <a href="./stats/quantile.html">quantile</a>
  </td>
  <td><h3>Sample Quantiles</h3>

<p>The generic function quantile produces sample quantiles corresponding 
 to the given probabilities. The smallest observation corresponds to 
 a probability of 0 and the largest to a probability of 1.</p></td>
</tr>
<tr>
  <td id="median">
    <a href="./stats/median.html">median</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="level">
    <a href="./stats/level.html">level</a>
  </td>
  <td><p>get quantile levels</p></td>
</tr>
<tr>
  <td id="dist">
    <a href="./stats/dist.html">dist</a>
  </td>
  <td><h3>Distance Matrix Computation</h3>

<p>This function computes and returns the distance matrix computed by using 
 the specified distance measure to compute the distances between the rows 
 of a data matrix.</p></td>
</tr>
<tr>
  <td id="pt">
    <a href="./stats/pt.html">pt</a>
  </td>
  <td><h2>The Student t Distribution</h2>

<p>Density, distribution function, quantile function and random generation for the t distribution with df degrees of freedom (and optional non-centrality parameter ncp).</p></td>
</tr>
<tr>
  <td id="t.test">
    <a href="./stats/t.test.html">t.test</a>
  </td>
  <td><p>Student's t-Test
 
 Performs one and two sample t-tests on vectors of data.</p></td>
</tr>
<tr>
  <td id="fisher.test">
    <a href="./stats/fisher.test.html">fisher.test</a>
  </td>
  <td><p>Fisher's Exact Test for Count Data
 
 Performs Fisher's exact test for testing the null of independence 
 of rows and columns in a contingency table with fixed marginals.</p></td>
</tr>
<tr>
  <td id="chisq.test">
    <a href="./stats/chisq.test.html">chisq.test</a>
  </td>
  <td><h3>Pearson's Chi-squared Test for Count Data</h3>

<p>chisq.test performs chi-squared contingency table tests and goodness-of-fit tests.</p></td>
</tr>
<tr>
  <td id="moran.test">
    <a href="./stats/moran.test.html">moran.test</a>
  </td>
  <td><p>Calculate Moran's I quickly for point data
 
 test spatial cluster via moran index</p></td>
</tr>
<tr>
  <td id="mantel.test">
    <a href="./stats/mantel.test.html">mantel.test</a>
  </td>
  <td><p>The Mantel test, named after Nathan Mantel, is a statistical test of 
 the correlation between two matrices. The matrices must be of the same
 dimension; in most applications, they are matrices of interrelations 
 between the same vectors of objects. The test was first published by 
 Nathan Mantel, a biostatistician at the National Institutes of Health, 
 in 1967.[1] Accounts of it can be found in advanced statistics books 
 (e.g., Sokal & Rohlf 1995[2]).</p></td>
</tr>
<tr>
  <td id="lowess">
    <a href="./stats/lowess.html">lowess</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="var.test">
    <a href="./stats/var.test.html">var.test</a>
  </td>
  <td><h2>F Test to Compare Two Variances</h2>

<p>Performs an F test to compare the variances of
 two samples from normal populations.</p></td>
</tr>
<tr>
  <td id="aov">
    <a href="./stats/aov.html">aov</a>
  </td>
  <td><h2>Fit an Analysis of Variance Model</h2>

<p>Fit an analysis of variance model by a call to lm for each stratum.</p></td>
</tr>
<tr>
  <td id="filterMissing">
    <a href="./stats/filterMissing.html">filterMissing</a>
  </td>
  <td><p>set the NA, NaN, Inf value to the default value</p></td>
</tr>
<tr>
  <td id="opls">
    <a href="./stats/opls.html">opls</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="cmdscale">
    <a href="./stats/cmdscale.html">cmdscale</a>
  </td>
  <td><h3>Classical (Metric) Multidimensional Scaling</h3>

<p>Classical multidimensional scaling (MDS) of a data matrix. Also known as principal coordinates analysis (Gower, 1966).</p></td>
</tr>
<tr>
  <td id="plsda">
    <a href="./stats/plsda.html">plsda</a>
  </td>
  <td><h2>Partial Least Squares Discriminant Analysis</h2>

<p><code>plsda</code> is used to calibrate, validate and use of partial least squares discrimination analysis (PLS-DA) model.</p></td>
</tr>
<tr>
  <td id="z">
    <a href="./stats/z.html">z</a>
  </td>
  <td><p>z-score</p></td>
</tr>
<tr>
  <td id="chi_square">
    <a href="./stats/chi_square.html">chi_square</a>
  </td>
  <td><p>The chiSquare method is used to determine whether there is a significant difference between the expected
 frequencies and the observed frequencies in one or more categories. It takes a double input x and an integer freedom
 for degrees of freedom as inputs. It returns the Chi Squared result.</p></td>
</tr>
<tr>
  <td id="gamma.cdf">
    <a href="./stats/gamma.cdf.html">gamma.cdf</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="gamma">
    <a href="./stats/gamma.html">gamma</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="lgamma">
    <a href="./stats/lgamma.html">lgamma</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="beta">
    <a href="./stats/beta.html">beta</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="lbeta">
    <a href="./stats/lbeta.html">lbeta</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="iqr_outliers">
    <a href="./stats/iqr_outliers.html">iqr_outliers</a>
  </td>
  <td><p>check of the outliers via IQR method</p></td>
</tr>
<tr>
  <td id="poisson_disk">
    <a href="./stats/poisson_disk.html">poisson_disk</a>
  </td>
  <td><p>Fast Poisson Disk Sampling in Arbitrary Dimensions. Robert Bridson. ACM SIGGRAPH 2007</p></td>
</tr>
<tr>
  <td id="kurtosis">
    <a href="./stats/kurtosis.html">kurtosis</a>
  </td>
  <td><p><strong>Kurtosis</strong> is a statistical measure that describes the "tailedness" of the probability distribution of a
 real-valued random variable. In simpler terms, it indicates the extent to which the tails of the distribution 
 differ from those of a normal distribution.
 
 ### Key Points about Kurtosis:
 
 1. <strong>Definition</strong>:
 
    - Kurtosis is the fourth standardized moment of a distribution.
    - It is calculated as the average of the squared deviations of the data from its mean, raised to the fourth power, standardized by the standard deviation raised to the fourth power.
    
 2. <strong>Types of Kurtosis</strong>:
 
    - <strong>Mesokurtic</strong>: Distributions with kurtosis similar to that of the normal distribution (kurtosis value of 3). The tails of a mesokurtic distribution are neither particularly fat nor particularly thin.
    - <strong>Leptokurtic</strong>: Distributions with positive kurtosis greater than 3. These distributions have "fat tails" and a sharp peak, indicating more frequent large deviations from the mean than a normal distribution.
    - <strong>Platykurtic</strong>: Distributions with kurtosis less than 3. These distributions have "thin tails" and a flatter peak, indicating fewer large deviations from the mean than a normal distribution.
    
 3. <strong>Excess Kurtosis</strong>:
 
    - Often, kurtosis is reported as "excess kurtosis," which is the kurtosis value minus 3. This adjustment makes the kurtosis of a normal distribution equal to 0.
    - Positive excess kurtosis indicates a leptokurtic distribution, while negative excess kurtosis indicates a platykurtic distribution.
    
 4. <strong>Interpretation</strong>:
 
    - High kurtosis in a data set is an indicator that data has heavy tails or outliers. This can affect the performance of statistical models and methods that assume normality.
    - Low kurtosis indicates that the data has light tails and lacks outliers.
    
 5. <strong>Applications</strong>:
 
    - In finance, kurtosis is used to describe the distribution of returns of an investment. A high kurtosis indicates a higher risk of extreme returns.
    - In data analysis, kurtosis helps in understanding the shape of the data distribution and identifying potential outliers.
    
 6. <strong>Calculation in R</strong>:
 
    - The `kurtosis()` function in the `e1071` package can be used to calculate kurtosis in R.
    - Alternatively, kurtosis can be calculated manually using the formula:

<pre><code class="R"> kurtosis &lt;- sum((data - mean(data))^4) / ((length(data) - 1) * sd(data)^4) - 3
</code></pre>


 
 kurtosis is a statistical measure for understanding the shape of a data distribution, particularly the behavior 
 of its tails. It is widely used in various fields, including finance, data analysis, and statistics.</p></td>
</tr>
<tr>
  <td id="skewness">
    <a href="./stats/skewness.html">skewness</a>
  </td>
  <td><p><strong>Skewness</strong>
 
 Skewness is a fundamental statistical measure used to describe the asymmetry of the probability distribution of a 
 real-valued random variable. It provides insights into the direction and extent of the deviation from a symmetric 
 distribution.
 
 ### Key Aspects of Skewness:
 
 1. <strong>Definition</strong>:
 
    - Skewness is the third standardized moment of a distribution.
    - It is calculated as the average of the cubed deviations of the data from its mean, standardized by the standard deviation raised to the third power.
    
 2. <strong>Types of Skewness</strong>:
 
    - <strong>Zero Skewness</strong>: Indicates a symmetric distribution where the mean, median, and mode are all equal.
    - <strong>Positive Skewness (Right-Skewed)</strong>: The tail on the right side of the distribution is longer or fatter. In this case, the mean is greater than the median.
    - <strong>Negative Skewness (Left-Skewed)</strong>: The tail on the left side of the distribution is longer or fatter. Here, the mean is less than the median.
    
 3. <strong>Interpretation</strong>:
 
    - Skewness values close to zero suggest a nearly symmetric distribution.
    - Positive values indicate right-skewed distributions, while negative values indicate left-skewed distributions.
    - The magnitude of the skewness value reflects the degree of asymmetry.
    
 4. <strong>Applications</strong>:
 
    - <strong>Finance</strong>: Used to analyze the distribution of returns on investments, helping investors understand the potential for extreme outcomes.
    - <strong>Economics</strong>: Assists in examining income distributions, enabling economists to assess income inequality.
    - <strong>Natural Sciences</strong>: Describes the distribution of experimental data in scientific research.
    
 5. <strong>Considerations</strong>:
 
    - Skewness is just one aspect of distribution shape and should be considered alongside other statistical measures like kurtosis for a comprehensive understanding.
    - For small sample sizes, the estimation of skewness can be unreliable.
    
 In essence, skewness is a statistical tool for understanding the asymmetry of data distributions, 
 with wide-ranging applications in various fields such as finance, economics, and the natural 
 sciences.</p></td>
</tr>
<tr>
  <td id="product_moments">
    <a href="./stats/product_moments.html">product_moments</a>
  </td>
  <td><p>In statistics, moments are a set of numerical characteristics that describe the shape and features of a probability distribution. 
 Sample moments are the same concept applied to a sample of data, rather than an entire population. They are used to estimate 
 the corresponding population moments and to understand the properties of the data distribution.
 
 Here's a basic introduction to the concept of sample moments:
 
 ### Definition:
 
 1. <strong>Sample Mean (First Moment):</strong>
    The sample mean is the average of the data points in a sample. It is a measure of the central tendency of the data.
      \[
      \bar{x} = \frac{1}{n} \sum<em>{i=1}^{n} x</em>i
      \]
    where \( x_i \) are the data points and \( n \) is the number of data points in the sample.
 2. <strong>Sample Variance (Second Central Moment):</strong>
    The sample variance measures the spread or dispersion of the data points around the sample mean.
      \[
      s^2 = \frac{1}{n-1} \sum<em>{i=1}^{n} (x</em>i - \bar{x})^2
      \]
    The denominator \( n-1 \) is used instead of \( n \) to provide an unbiased estimate of the population variance.
 3. <strong>Sample Standard Deviation:</strong>
    The sample standard deviation is the square root of the sample variance and is also a measure of dispersion.
      \[
      s = \sqrt{s^2}
      \]
 4. <strong>Higher-Order Sample Moments:</strong>
    Higher-order moments describe the shape of the distribution. For example:
    - <strong>Third Moment:</strong> Measures skewness, which indicates the asymmetry of the data distribution.
    - <strong>Fourth Moment:</strong> Measures kurtosis, which indicates the "tailedness" of the data distribution.
    
 ### Calculation:
 
 To calculate sample moments, you simply apply the formulas to your data set. For instance, to find the sample mean,
 you add up all the data points and divide by the number of points.
 
 ### Use:
 
 Sample moments are used to:
 - Estimate population parameters.
 - Assess the shape of the data distribution (e.g., normality, skewness, kurtosis).
 - Form the basis for many statistical tests and procedures.
 
 ### Properties:
 - <strong>Unbiasedness:</strong> Some sample moments are designed to be unbiased estimators, meaning that the expected value of the sample moment equals the population moment.
 - <strong>Efficiency:</strong> Different sample moments may have different levels of variability; some are more efficient than others.
 - <strong>Robustness:</strong> Certain moments are more robust to outliers than others.
 
 ### Example:
 If you have a sample of data: \( \{2, 4, 4, 4, 5, 5, 7, 9\} \), you can calculate the sample mean, variance, 
 and other moments to understand the central tendency, dispersion, and shape of the data distribution.
 
 sample moments are fundamental tools in statistics for summarizing and understanding the characteristics of
 a data set. They provide a way to quantify features such as location, spread, and shape, which are essential 
 for further statistical analysis.
 
 @author Will<em>and</em>Sara</p></td>
</tr>
<tr>
  <td id="moment">
    <a href="./stats/moment.html">moment</a>
  </td>
  <td><h3>Statistical Moments</h3>

<p>This function computes the sample moment of specified order.</p></td>
</tr>
<tr>
  <td id="emd_dist">
    <a href="./stats/emd_dist.html">emd_dist</a>
  </td>
  <td><h3>Earth Mover's Distance</h3>

<p>Implementation of the Fast Earth Mover's Algorithm by Ofir Pele and Michael Werman.</p></td>
</tr></tbody>
      </table>
    </div>
    <hr />
    <div style="text-align: center;">[<a href="../index.html">Document Index</a>]</div>
  </body>
</html>